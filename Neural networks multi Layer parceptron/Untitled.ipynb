{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "5            5      116             74              0        0  25.6   \n",
      "6            3       78             50             32       88  31.0   \n",
      "7           10      115              0              0        0  35.3   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n",
      "5                     0.201   30        0  \n",
      "6                     0.248   26        1  \n",
      "7                     0.134   29        0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "da = df.dropna()\n",
    "print(df.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = MLPClassifier(verbose=True, solver='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,0:8]\n",
    "y = df['Outcome']\n",
    "# print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69284145\n",
      "Iteration 2, loss = 0.69076476\n",
      "Iteration 3, loss = 0.68764897\n",
      "Iteration 4, loss = 0.68389859\n",
      "Iteration 5, loss = 0.67961608\n",
      "Iteration 6, loss = 0.67496814\n",
      "Iteration 7, loss = 0.67044295\n",
      "Iteration 8, loss = 0.66584029\n",
      "Iteration 9, loss = 0.66138838\n",
      "Iteration 10, loss = 0.65730638\n",
      "Iteration 11, loss = 0.65320897\n",
      "Iteration 12, loss = 0.64921185\n",
      "Iteration 13, loss = 0.64544277\n",
      "Iteration 14, loss = 0.64174241\n",
      "Iteration 15, loss = 0.63803568\n",
      "Iteration 16, loss = 0.63442304\n",
      "Iteration 17, loss = 0.63107382\n",
      "Iteration 18, loss = 0.62777058\n",
      "Iteration 19, loss = 0.62457435\n",
      "Iteration 20, loss = 0.62138715\n",
      "Iteration 21, loss = 0.61836598\n",
      "Iteration 22, loss = 0.61549764\n",
      "Iteration 23, loss = 0.61263269\n",
      "Iteration 24, loss = 0.60998898\n",
      "Iteration 25, loss = 0.60743492\n",
      "Iteration 26, loss = 0.60494978\n",
      "Iteration 27, loss = 0.60247133\n",
      "Iteration 28, loss = 0.60004596\n",
      "Iteration 29, loss = 0.59765457\n",
      "Iteration 30, loss = 0.59528449\n",
      "Iteration 31, loss = 0.59287397\n",
      "Iteration 32, loss = 0.59071926\n",
      "Iteration 33, loss = 0.58837951\n",
      "Iteration 34, loss = 0.58628019\n",
      "Iteration 35, loss = 0.58410487\n",
      "Iteration 36, loss = 0.58203853\n",
      "Iteration 37, loss = 0.58013864\n",
      "Iteration 38, loss = 0.57830518\n",
      "Iteration 39, loss = 0.57644372\n",
      "Iteration 40, loss = 0.57458933\n",
      "Iteration 41, loss = 0.57274860\n",
      "Iteration 42, loss = 0.57107036\n",
      "Iteration 43, loss = 0.56936577\n",
      "Iteration 44, loss = 0.56779411\n",
      "Iteration 45, loss = 0.56616392\n",
      "Iteration 46, loss = 0.56449485\n",
      "Iteration 47, loss = 0.56290954\n",
      "Iteration 48, loss = 0.56137564\n",
      "Iteration 49, loss = 0.55990697\n",
      "Iteration 50, loss = 0.55842091\n",
      "Iteration 51, loss = 0.55692714\n",
      "Iteration 52, loss = 0.55545834\n",
      "Iteration 53, loss = 0.55402170\n",
      "Iteration 54, loss = 0.55262301\n",
      "Iteration 55, loss = 0.55126297\n",
      "Iteration 56, loss = 0.54996273\n",
      "Iteration 57, loss = 0.54874338\n",
      "Iteration 58, loss = 0.54751110\n",
      "Iteration 59, loss = 0.54629169\n",
      "Iteration 60, loss = 0.54497777\n",
      "Iteration 61, loss = 0.54361462\n",
      "Iteration 62, loss = 0.54242075\n",
      "Iteration 63, loss = 0.54120774\n",
      "Iteration 64, loss = 0.54005207\n",
      "Iteration 65, loss = 0.53888740\n",
      "Iteration 66, loss = 0.53774025\n",
      "Iteration 67, loss = 0.53661882\n",
      "Iteration 68, loss = 0.53556988\n",
      "Iteration 69, loss = 0.53442518\n",
      "Iteration 70, loss = 0.53341709\n",
      "Iteration 71, loss = 0.53249505\n",
      "Iteration 72, loss = 0.53162695\n",
      "Iteration 73, loss = 0.53068999\n",
      "Iteration 74, loss = 0.52977167\n",
      "Iteration 75, loss = 0.52884374\n",
      "Iteration 76, loss = 0.52794584\n",
      "Iteration 77, loss = 0.52706039\n",
      "Iteration 78, loss = 0.52619230\n",
      "Iteration 79, loss = 0.52536277\n",
      "Iteration 80, loss = 0.52456357\n",
      "Iteration 81, loss = 0.52372224\n",
      "Iteration 82, loss = 0.52297669\n",
      "Iteration 83, loss = 0.52218330\n",
      "Iteration 84, loss = 0.52141564\n",
      "Iteration 85, loss = 0.52069736\n",
      "Iteration 86, loss = 0.52003795\n",
      "Iteration 87, loss = 0.51940562\n",
      "Iteration 88, loss = 0.51885143\n",
      "Iteration 89, loss = 0.51824064\n",
      "Iteration 90, loss = 0.51759674\n",
      "Iteration 91, loss = 0.51697589\n",
      "Iteration 92, loss = 0.51622907\n",
      "Iteration 93, loss = 0.51555141\n",
      "Iteration 94, loss = 0.51489497\n",
      "Iteration 95, loss = 0.51425628\n",
      "Iteration 96, loss = 0.51363517\n",
      "Iteration 97, loss = 0.51311744\n",
      "Iteration 98, loss = 0.51258107\n",
      "Iteration 99, loss = 0.51202915\n",
      "Iteration 100, loss = 0.51139574\n",
      "Iteration 101, loss = 0.51081184\n",
      "Iteration 102, loss = 0.51034360\n",
      "Iteration 103, loss = 0.50979728\n",
      "Iteration 104, loss = 0.50928267\n",
      "Iteration 105, loss = 0.50880408\n",
      "Iteration 106, loss = 0.50831917\n",
      "Iteration 107, loss = 0.50783436\n",
      "Iteration 108, loss = 0.50730004\n",
      "Iteration 109, loss = 0.50681044\n",
      "Iteration 110, loss = 0.50634297\n",
      "Iteration 111, loss = 0.50594806\n",
      "Iteration 112, loss = 0.50550683\n",
      "Iteration 113, loss = 0.50503829\n",
      "Iteration 114, loss = 0.50465155\n",
      "Iteration 115, loss = 0.50419067\n",
      "Iteration 116, loss = 0.50373792\n",
      "Iteration 117, loss = 0.50324640\n",
      "Iteration 118, loss = 0.50281860\n",
      "Iteration 119, loss = 0.50230770\n",
      "Iteration 120, loss = 0.50184587\n",
      "Iteration 121, loss = 0.50132900\n",
      "Iteration 122, loss = 0.50084377\n",
      "Iteration 123, loss = 0.50038388\n",
      "Iteration 124, loss = 0.49992569\n",
      "Iteration 125, loss = 0.49952960\n",
      "Iteration 126, loss = 0.49915373\n",
      "Iteration 127, loss = 0.49874381\n",
      "Iteration 128, loss = 0.49834810\n",
      "Iteration 129, loss = 0.49797188\n",
      "Iteration 130, loss = 0.49758224\n",
      "Iteration 131, loss = 0.49718025\n",
      "Iteration 132, loss = 0.49680261\n",
      "Iteration 133, loss = 0.49643572\n",
      "Iteration 134, loss = 0.49608215\n",
      "Iteration 135, loss = 0.49572519\n",
      "Iteration 136, loss = 0.49534049\n",
      "Iteration 137, loss = 0.49501584\n",
      "Iteration 138, loss = 0.49464744\n",
      "Iteration 139, loss = 0.49428500\n",
      "Iteration 140, loss = 0.49392672\n",
      "Iteration 141, loss = 0.49360569\n",
      "Iteration 142, loss = 0.49327228\n",
      "Iteration 143, loss = 0.49298365\n",
      "Iteration 144, loss = 0.49267111\n",
      "Iteration 145, loss = 0.49235881\n",
      "Iteration 146, loss = 0.49206770\n",
      "Iteration 147, loss = 0.49169431\n",
      "Iteration 148, loss = 0.49136958\n",
      "Iteration 149, loss = 0.49105402\n",
      "Iteration 150, loss = 0.49074632\n",
      "Iteration 151, loss = 0.49046315\n",
      "Iteration 152, loss = 0.49014088\n",
      "Iteration 153, loss = 0.48985248\n",
      "Iteration 154, loss = 0.48952503\n",
      "Iteration 155, loss = 0.48922337\n",
      "Iteration 156, loss = 0.48889310\n",
      "Iteration 157, loss = 0.48860172\n",
      "Iteration 158, loss = 0.48834292\n",
      "Iteration 159, loss = 0.48798723\n",
      "Iteration 160, loss = 0.48773651\n",
      "Iteration 161, loss = 0.48745659\n",
      "Iteration 162, loss = 0.48723175\n",
      "Iteration 163, loss = 0.48699796\n",
      "Iteration 164, loss = 0.48681760\n",
      "Iteration 165, loss = 0.48657582\n",
      "Iteration 166, loss = 0.48636100\n",
      "Iteration 167, loss = 0.48617756\n",
      "Iteration 168, loss = 0.48606240\n",
      "Iteration 169, loss = 0.48582195\n",
      "Iteration 170, loss = 0.48567887\n",
      "Iteration 171, loss = 0.48551025\n",
      "Iteration 172, loss = 0.48528683\n",
      "Iteration 173, loss = 0.48508665\n",
      "Iteration 174, loss = 0.48482953\n",
      "Iteration 175, loss = 0.48463751\n",
      "Iteration 176, loss = 0.48438629\n",
      "Iteration 177, loss = 0.48414052\n",
      "Iteration 178, loss = 0.48389424\n",
      "Iteration 179, loss = 0.48368991\n",
      "Iteration 180, loss = 0.48354871\n",
      "Iteration 181, loss = 0.48329947\n",
      "Iteration 182, loss = 0.48303304\n",
      "Iteration 183, loss = 0.48283504\n",
      "Iteration 184, loss = 0.48263006\n",
      "Iteration 185, loss = 0.48244667\n",
      "Iteration 186, loss = 0.48227606\n",
      "Iteration 187, loss = 0.48208485\n",
      "Iteration 188, loss = 0.48192822\n",
      "Iteration 189, loss = 0.48173001\n",
      "Iteration 190, loss = 0.48157587\n",
      "Iteration 191, loss = 0.48141232\n",
      "Iteration 192, loss = 0.48120849\n",
      "Iteration 193, loss = 0.48100666\n",
      "Iteration 194, loss = 0.48082293\n",
      "Iteration 195, loss = 0.48063961\n",
      "Iteration 196, loss = 0.48048392\n",
      "Iteration 197, loss = 0.48033821\n",
      "Iteration 198, loss = 0.48015331\n",
      "Iteration 199, loss = 0.47999838\n",
      "Iteration 200, loss = 0.47982922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y,random_state = 0, test_size= 0.2)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "xtrain = sc.fit_transform(xtrain)\n",
    "xtest = sc.fit_transform(xtest)\n",
    "al.fit(xtrain,ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp = al.predict(xtest)\n",
    "yp.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93 14]\n",
      " [19 28]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(ytest,yp)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in percentage is :>78.57142857142857 % \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"The accuracy in percentage is :>{} % \".format((accuracy_score(ytest, yp))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xtest)\n",
    "# xtest = sc.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Pregnancies3\n",
      "Enter Glucose43\n",
      "Enter BloodPressure34\n",
      "Enter SkinThickness43\n",
      "Enter Insulin34\n",
      "Enter BMI34\n",
      "Enter DiabetesPedigreeFunction43\n",
      "Enter Age34\n",
      "you don't have Diabetes\n"
     ]
    }
   ],
   "source": [
    "a=int(input(\"Enter Pregnancies\"))\n",
    "b=int(input(\"Enter Glucose\"))\n",
    "c=int(input(\"Enter BloodPressure\"))\n",
    "d=int(input(\"Enter SkinThickness\"))\n",
    "e=int(input(\"Enter Insulin\"))\n",
    "f=float(input(\"Enter BMI\"))\n",
    "g=float(input(\"Enter DiabetesPedigreeFunction\"))\n",
    "h=int(input(\"Enter Age\"))\n",
    "l=[[a,b,c,d,e,f,g,h]]\n",
    "l1=np.array(l)\n",
    "l1 = sc.fit_transform(l1)\n",
    "yp = al.predict(l1)\n",
    "if(yp==1):\n",
    "    print(\"You have Diabetes\")\n",
    "else:\n",
    "    print(\"you don't have Diabetes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
